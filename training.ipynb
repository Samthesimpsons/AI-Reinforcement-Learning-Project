{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "from models.wordle_base_15k import run_simulations as rl_base\n",
    "from models.wordle_cluster_15k import run_simulations as rl_cluster\n",
    "from models.wordle_cluster_2k import run_simulations as rl_cluster_2\n",
    "import pandas as pd\n",
    "\n",
    "# Our hyper-parameters to search on\n",
    "learning_rates = [0.1, 0.01, 0.001]    \n",
    "exploration_rates = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "shrinkage_factors = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "num_of_clusters = [6, 7, 8, 9, 10]\n",
    "\n",
    "# Set to True to rerun grid_search\n",
    "run_grid_search = False\n",
    "\n",
    "# Grid-search function for RL-base 15k\n",
    "def func_1():\n",
    "    df_1 = pd.DataFrame(columns=['learning_rate', 'exploration_rate', 'shrinkage_factor', 'time_taken', 'average_guesses', 'win_rate'])\n",
    "\n",
    "    for i, alpha in enumerate(learning_rates):\n",
    "        for j, epsilon in enumerate(exploration_rates):\n",
    "            for k, gamma in enumerate(shrinkage_factors):\n",
    "                print(f'Running epoch {i}, {j}, {k}')\n",
    "                time_taken, average_guesses, win_rate, guesses = rl_base(learning_rate=alpha, exploration_rate=epsilon, shrinkage_factor=gamma, num_simulations=100)\n",
    "                df_1.loc[len(df_1)] = [alpha, epsilon, gamma, time_taken, average_guesses, win_rate]\n",
    "                \n",
    "    df_1.to_csv('grid_search_results/results_base.csv', index=False)\n",
    "    df_1 = df_1.sort_values(by=['win_rate', 'average_guesses', 'time_taken'], ascending=[False, True, True])\n",
    "    print(df_1.iloc[0])\n",
    "\n",
    "# Grid-search function for RL with clustering 15k\n",
    "def func_2():\n",
    "    df_2 = pd.DataFrame(columns=['learning_rate', 'exploration_rate', 'shrinkage_factor', 'num_of_clusters', 'time_taken', 'average_guesses', 'win_rate'])\n",
    "\n",
    "    for i, alpha in enumerate(learning_rates):\n",
    "        for j, epsilon in enumerate(exploration_rates):\n",
    "            for k, gamma in enumerate(shrinkage_factors):\n",
    "                for l, num_clusters in enumerate(num_of_clusters):\n",
    "                    print(f'Running epoch {i}, {j}, {k}, {l}')\n",
    "                    time_taken, average_guesses, win_rate, guesses = rl_cluster(learning_rate=alpha, exploration_rate=epsilon, shrinkage_factor=gamma, number_of_cluster=num_clusters, num_simulations=100)\n",
    "                    df_2.loc[len(df_2)] = [alpha, epsilon, gamma, num_clusters, time_taken, average_guesses, win_rate]\n",
    "\n",
    "    df_2.to_csv('grid_search_results/results_cluster.csv', index=False)\n",
    "    df_2 = df_2.sort_values(by=['win_rate', 'average_guesses', 'time_taken'], ascending=[False, True, True])\n",
    "    print(df_2.iloc[0])\n",
    "\n",
    "# Grid-search function for RL with clustering 2k\n",
    "def func_3():\n",
    "    df_3 = pd.DataFrame(columns=['learning_rate', 'exploration_rate', 'shrinkage_factor', 'num_of_clusters', 'time_taken', 'average_guesses', 'win_rate'])\n",
    "\n",
    "    for i, alpha in enumerate(learning_rates):\n",
    "        for j, epsilon in enumerate(exploration_rates):\n",
    "            for k, gamma in enumerate(shrinkage_factors):\n",
    "                for l,num_clusters in enumerate(num_of_clusters):\n",
    "                    print(f'Running epoch {i}, {j}, {k}, {l}')\n",
    "                    time_taken, average_guesses, win_rate, guesses = rl_cluster_2(learning_rate=alpha, exploration_rate=epsilon, shrinkage_factor=gamma, number_of_cluster=num_clusters, num_simulations=100)\n",
    "                    df_3.loc[len(df_3)] = [alpha, epsilon, gamma, num_clusters, time_taken, average_guesses, win_rate]\n",
    "\n",
    "    df_3.to_csv('grid_search_results/results_cluster_2.csv', index=False)\n",
    "    df_3 = df_3.sort_values(by=['win_rate', 'average_guesses', 'time_taken'], ascending=[False, True, True])\n",
    "    print(df_3.iloc[0])\n",
    "\n",
    "# Running the grid-search functions in cpu parallel\n",
    "def run_cpu_tasks_in_parallel(tasks):\n",
    "    running_tasks = [Process(target=task) for task in tasks]\n",
    "    for running_task in running_tasks:\n",
    "        running_task.start()\n",
    "    for running_task in running_tasks:\n",
    "        running_task.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_grid_search == True:\n",
    "    run_cpu_tasks_in_parallel([func_1, func_2, func_3])\n",
    "\n",
    "df_1 = pd.read_csv('grid_search_results/results_base.csv')\n",
    "df_2 = pd.read_csv('grid_search_results/results_cluster.csv')\n",
    "df_3 = pd.read_csv('grid_search_results/results_cluster_2.csv')\n",
    "\n",
    "df_1 = df_1.sort_values(by=['win_rate', 'average_guesses', 'time_taken'], ascending=[False, True, True])\n",
    "df_2 = df_2.sort_values(by=['win_rate', 'average_guesses', 'time_taken'], ascending=[False, True, True])\n",
    "df_3 = df_3.sort_values(by=['win_rate', 'average_guesses', 'time_taken'], ascending=[False, True, True])\n",
    "\n",
    "print(df_1.iloc[0])\n",
    "print(df_2.iloc[0])\n",
    "print(df_3.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation \n",
    "\n",
    "Run each of the 5 models, using the default hyper-parameter values based off the grid-search, over 10000 simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from math import log10, floor\n",
    "from scipy.stats import t\n",
    "import matplotlib.pyplot as plt\n",
    "from models.wordle_base_15k import run_simulations as rl_base\n",
    "from models.wordle_cluster_15k import run_simulations as rl_cluster\n",
    "from models.wordle_cluster_2k import run_simulations as rl_cluster_2\n",
    "from models.wordle_greedy_search_15k import run_simulations as greed_search\n",
    "from models.wordle_greedy_search_2k import run_simulations as greed_search_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the 10000 simulations per model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running 10000 simulations for each of the 5 models and saving the results\n",
    "run = False\n",
    "if run:\n",
    "    results = pd.DataFrame(columns=['time_taken', 'average_guesses', 'win_rate'])\n",
    "\n",
    "    for i in tqdm(range(5)):\n",
    "        if i == 0:\n",
    "            time_taken, average_guesses, win_rate, guesses = rl_base(learning_rate=0.1, exploration_rate=0.8, shrinkage_factor=0.8, num_simulations=10000)\n",
    "            base_guesses = guesses\n",
    "            results.loc[0] = [time_taken, average_guesses, win_rate]\n",
    "        elif i == 1:\n",
    "            time_taken, average_guesses, win_rate, guesses = rl_cluster(learning_rate=0.1, exploration_rate=0.5, number_of_cluster=6 ,shrinkage_factor=0.9, num_simulations=10000)\n",
    "            cluster_guesses = guesses\n",
    "            results.loc[1] = [time_taken, average_guesses, win_rate]\n",
    "        elif i == 2:\n",
    "            time_taken, average_guesses, win_rate, guesses = rl_cluster_2(learning_rate=0.001, exploration_rate=0.9, number_of_cluster=9, shrinkage_factor=0.9, num_simulations=10000)\n",
    "            cluster_2_guesses = guesses\n",
    "            results.loc[2] = [time_taken, average_guesses, win_rate]\n",
    "        elif i == 3:\n",
    "            time_taken, average_guesses, win_rate, guesses = greed_search(num_simulations=10000)\n",
    "            greedy_search_guesses = guesses\n",
    "            results.loc[3] = [time_taken, average_guesses, win_rate]\n",
    "        elif i == 4:\n",
    "            time_taken, average_guesses, win_rate, guesses = greed_search_2(num_simulations=10000)\n",
    "            greedy_search_2_guesses = guesses\n",
    "            results.loc[4] = [time_taken, average_guesses, win_rate]\n",
    "\n",
    "    results.to_csv('evaluation_results/results.csv')\n",
    "    np.save('evaluation_results/base_guesses.npy', base_guesses)\n",
    "    np.save('evaluation_results/cluster_guesses.npy', cluster_guesses)\n",
    "    np.save('evaluation_results/cluster_2_guesses.npy', cluster_2_guesses)\n",
    "    np.save('evaluation_results/greedy_search_guesses.npy', greedy_search_guesses)\n",
    "    np.save('evaluation_results/greedy_search_2_guesses.npy', greedy_search_2_guesses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the saved results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the results\n",
    "results = pd.read_csv('evaluation_results/results.csv')\n",
    "base_guesses = np.load('evaluation_results/base_guesses.npy')\n",
    "cluster_guesses = np.load('evaluation_results/cluster_guesses.npy')\n",
    "cluster_2_guesses = np.load('evaluation_results/cluster_2_guesses.npy')\n",
    "greedy_search_guesses = np.load('evaluation_results/greedy_search_guesses.npy')\n",
    "greedy_search_2_guesses = np.load('evaluation_results/greedy_search_2_guesses.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for burn-in period and if the Q-learning reduces the guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the guesses over time to check for burn-in period and to check if the Q-learning reduces the guesses. \n",
    "epochs = np.arange(10000)\n",
    "fig, ax = plt.subplots(3, 2, figsize=(10, 10))\n",
    "ax[0, 0].bar(epochs, base_guesses, color='blue')\n",
    "ax[0, 0].set_title('RL Base 15k Words')\n",
    "ax[1, 0].bar(epochs, cluster_guesses, color='blue')\n",
    "ax[1, 0].set_title('RL with Cluster 15k Words')\n",
    "ax[1, 1].bar(epochs, cluster_2_guesses, color='blue')\n",
    "ax[1, 1].set_title('RL with Cluster 2k Words')\n",
    "ax[2, 0].bar(epochs, greedy_search_guesses, color='blue')\n",
    "ax[2, 0].set_title('Greedy Search 15k Words')\n",
    "ax[2, 1].bar(epochs, greedy_search_2_guesses, color='blue')\n",
    "ax[2, 1].set_title('Greedy Search 2k Words')\n",
    "ax[0, 1].set_axis_off()\n",
    "fig.supxlabel('Run Index')\n",
    "fig.supylabel('Number of Guesses required to get goal word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the batch-avg calculations with burn-in period of 1000 aka remove first batch for average guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch-averaging calculations with burn-in period of 1000\n",
    "def batch_calc(guesses:np.ndarray): \n",
    "    guesses_mean = []\n",
    "    for i in range(0, len(guesses), 1000):\n",
    "        guesses_mean.append(np.mean(guesses[i:i+1000]))\n",
    "    guesses_mean = np.array(guesses_mean)\n",
    "    guesses_mean = guesses_mean[1:]\n",
    "    return guesses_mean\n",
    "\n",
    "# Round to nearest 3sf\n",
    "def round_sig(x, sig=3):\n",
    "    return round(x, sig-int(floor(log10(abs(x))))-1)\n",
    "\n",
    "# Find the confidence interval for the batch-averaged guesses\n",
    "def calculate_t_test_CI(values, confidence:int):\n",
    "    dof = len(values) - 1\n",
    "    conf = confidence\n",
    "    t_crit = np.abs(t.ppf((1-confidence)/2,dof))\n",
    "    mean = np.mean(values)\n",
    "    variance = np.var(values)\n",
    "    return (round((mean - t_crit * (variance / np.sqrt(dof))),3), round((mean + t_crit * (variance / np.sqrt(dof))),3))\n",
    "\n",
    "# Do the batch-averaging calculations\n",
    "base_avg_guesses = batch_calc(base_guesses)\n",
    "cluster_avg_guesses = batch_calc(cluster_guesses)\n",
    "cluster_2_avg_guesses = batch_calc(cluster_2_guesses)\n",
    "greedy_search_avg_guesses = batch_calc(greedy_search_guesses)\n",
    "greedy_search_2_avg_guesses = batch_calc(greedy_search_2_guesses)\n",
    "\n",
    "# Plot the batch-averaged guesses\n",
    "sns.lineplot(x=range(len(base_avg_guesses)), y=base_avg_guesses, marker='o', markersize=5, linewidth=1, label='RL base with 15k words')\n",
    "sns.lineplot(x=range(len(cluster_avg_guesses)), y=cluster_avg_guesses, marker='o', markersize=5, linewidth=1, label='RL with cluster 15k words')\n",
    "sns.lineplot(x=range(len(cluster_2_avg_guesses)), y=cluster_2_avg_guesses, marker='o', markersize=5, linewidth=1, label='RL with cluster 2k words')\n",
    "sns.lineplot(x=range(len(greedy_search_avg_guesses)), y=greedy_search_avg_guesses, marker='o', markersize=5, linewidth=1, label='Greedy Search 15k words')\n",
    "sns.lineplot(x=range(len(greedy_search_2_avg_guesses)), y=greedy_search_2_avg_guesses, marker='o', markersize=5, linewidth=1, label='Greedy Search 2k words')\n",
    "\n",
    "for i in range(len(base_avg_guesses)):\n",
    "    plt.text(i, base_avg_guesses[i], str(base_avg_guesses[i]), horizontalalignment='center', verticalalignment='bottom')\n",
    "    plt.text(i, cluster_avg_guesses[i], str(cluster_avg_guesses[i]), horizontalalignment='center', verticalalignment='bottom')\n",
    "    plt.text(i, cluster_2_avg_guesses[i], str(cluster_2_avg_guesses[i]), horizontalalignment='center', verticalalignment='bottom')\n",
    "    plt.text(i, greedy_search_avg_guesses[i], str(greedy_search_avg_guesses[i]), horizontalalignment='center', verticalalignment='bottom')\n",
    "    plt.text(i, greedy_search_2_avg_guesses[i], str(greedy_search_2_avg_guesses[i]), horizontalalignment='center', verticalalignment='bottom')\n",
    "    \n",
    "plt.title(\"Average guesses per 1000 simulations\")\n",
    "plt.xlabel(\"Batch number (1000 simulations per batch)\")\n",
    "plt.ylabel(\"Average guesses\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "# Command-line printing of the confidence interval for the batch-averaged guesses, and the average guesses mean and variance\n",
    "base_avg_guesses_mean = round_sig(np.mean(base_avg_guesses),3)\n",
    "cluster_avg_guesses_mean = round_sig(np.mean(cluster_avg_guesses),3)\n",
    "cluster_2_avg_guesses_mean = round_sig(np.mean(cluster_2_avg_guesses),3)\n",
    "greedy_search_avg_guesses_mean = round_sig(np.mean(greedy_search_avg_guesses),3)\n",
    "greedy_search_2_avg_guesses_mean = round_sig(np.mean(greedy_search_2_avg_guesses),3)\n",
    "\n",
    "base_avg_guesses_variance = round_sig(np.var(base_avg_guesses),3)\n",
    "cluster_avg_guesses_variance = round_sig(np.var(cluster_avg_guesses),3)\n",
    "cluster_2_avg_guesses_variance = round_sig(np.var(cluster_2_avg_guesses),3)\n",
    "greedy_search_avg_guesses_variance = round_sig(np.var(greedy_search_avg_guesses),3)\n",
    "greedy_search_2_avg_guesses_variance = round_sig(np.var(greedy_search_2_avg_guesses),3)\n",
    "\n",
    "base_avg_guesses_CI = calculate_t_test_CI(base_avg_guesses, 0.95)\n",
    "cluster_avg_guesses_CI = calculate_t_test_CI(cluster_avg_guesses, 0.95)\n",
    "cluster_2_avg_guesses_CI = calculate_t_test_CI(cluster_2_avg_guesses, 0.95)\n",
    "greedy_search_avg_guesses_CI = calculate_t_test_CI(greedy_search_avg_guesses, 0.95)\n",
    "greedy_search_2_avg_guesses_CI = calculate_t_test_CI(greedy_search_2_avg_guesses, 0.95)\n",
    "\n",
    "print(f'Steady state average guesses for RL base with 15k words is {base_avg_guesses_mean}, its variance is {base_avg_guesses_variance} and 95% confidence interval of the average guess is {base_avg_guesses_CI}.')\n",
    "print(f'Steady state average guesses for RL with clustering & 15k words is {cluster_avg_guesses_mean}, its variance is {cluster_avg_guesses_variance} and 95% confidence interval of the average guess is {cluster_avg_guesses_CI}.')\n",
    "print(f'Steady state average guesses for RL with clustering & 2k words is {cluster_2_avg_guesses_mean}, its variance is {cluster_2_avg_guesses_variance} and 95% confidence interval of the average guess is {cluster_2_avg_guesses_CI}.')\n",
    "print(f'Steady state average guesses for Greedy search with 15k words is {greedy_search_avg_guesses_mean}, its variance is {greedy_search_avg_guesses_variance} and 95% confidence interval of the average guess is {greedy_search_avg_guesses_CI}.')\n",
    "print(f'Steady state average guesses for Greedy search with 2k words is {greedy_search_2_avg_guesses_mean}, its variance is {greedy_search_2_avg_guesses_variance} and 95% confidence interval of the average guess is {greedy_search_2_avg_guesses_CI}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the batch-avg calculations with burn-in period of 1000 aka remove first batch for average win rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the winrate for each run\n",
    "def calculate_win_rate(batch_len:int, guesses):\n",
    "    win_rate = (batch_len-np.sum(guesses>6))/batch_len*100\n",
    "    return win_rate\n",
    "\n",
    "# Batch-averaging calculations with burn-in period of 1000\n",
    "def batch_calc(guesses:np.ndarray): \n",
    "    win_rate = []\n",
    "    for i in range(0, len(guesses), 1000):\n",
    "        win_rate.append(calculate_win_rate(1000,guesses[i:i+1000]))\n",
    "    win_rate = np.array(win_rate)\n",
    "    win_rate = win_rate[1:]\n",
    "    return win_rate\n",
    "\n",
    "# Round to nearest 3sf\n",
    "def round_sig(x, sig=3):\n",
    "    return round(x, sig-int(floor(log10(abs(x))))-1)\n",
    "\n",
    "# Find the confidence interval for the batch-averaged guesses\n",
    "def calculate_t_test_CI(values, confidence:int):\n",
    "    dof = len(values) - 1\n",
    "    conf = confidence\n",
    "    t_crit = np.abs(t.ppf((1-confidence)/2,dof))\n",
    "    mean = np.mean(values)\n",
    "    variance = np.var(values)\n",
    "    return (round((mean - t_crit * (variance / np.sqrt(dof))),1), round((mean + t_crit * (variance / np.sqrt(dof))),1))\n",
    "\n",
    "# Do the batch-averaging calculations\n",
    "base_avg_win_rates = batch_calc(base_guesses)\n",
    "cluster_avg_win_rates = batch_calc(cluster_guesses)\n",
    "cluster_2_avg_win_rates = batch_calc(cluster_2_guesses)\n",
    "greedy_search_avg_win_rates = batch_calc(greedy_search_guesses)\n",
    "greedy_search_2_avg_win_rates = batch_calc(greedy_search_2_guesses)\n",
    "\n",
    "# Plot the winrate for each batch\n",
    "sns.lineplot(x=range(len(base_avg_win_rates)), y=base_avg_win_rates, marker='o', markersize=5, linewidth=1, label='RL base with 15k words')\n",
    "sns.lineplot(x=range(len(cluster_avg_win_rates)), y=cluster_avg_win_rates, marker='o', markersize=5, linewidth=1, label='RL with clustering & 15k words')\n",
    "sns.lineplot(x=range(len(cluster_2_avg_win_rates)), y=cluster_2_avg_win_rates, marker='o', markersize=5, linewidth=1, label='RL with clustering & 2k words')\n",
    "sns.lineplot(x=range(len(greedy_search_avg_win_rates)), y=greedy_search_avg_win_rates, marker='o', markersize=5, linewidth=1, label='Greedy search with 15k words')\n",
    "sns.lineplot(x=range(len(greedy_search_2_avg_win_rates)), y=greedy_search_2_avg_win_rates, marker='o', markersize=5, linewidth=1, label='Greedy search with 2k words')\n",
    "\n",
    "for i in range(len(base_avg_win_rates)):\n",
    "    plt.text(i, base_avg_win_rates[i], str(round(base_avg_win_rates[i],3)), horizontalalignment='center', verticalalignment='bottom')\n",
    "    plt.text(i, cluster_avg_win_rates[i], str(round(cluster_avg_win_rates[i],3)), horizontalalignment='center', verticalalignment='bottom')\n",
    "    plt.text(i, cluster_2_avg_win_rates[i], str(round(cluster_2_avg_win_rates[i],3)), horizontalalignment='center', verticalalignment='bottom')\n",
    "    plt.text(i, greedy_search_avg_win_rates[i], str(round(greedy_search_avg_win_rates[i],3)), horizontalalignment='center', verticalalignment='bottom')\n",
    "    plt.text(i, greedy_search_2_avg_win_rates[i], str(round(greedy_search_2_avg_win_rates[i],3)), horizontalalignment='center', verticalalignment='bottom')\n",
    "    \n",
    "plt.title(\"Average win-rate per 1000 simulations\")\n",
    "plt.xlabel(\"Batch number (1000 simulations per batch)\")\n",
    "plt.ylabel(\"Average win-rate\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "\n",
    "# Command-line printing of the confidence interval for the batch-averaged win-rate, and the average win-rate mean and variance\n",
    "base_avg_win_rates_mean = round_sig(np.mean(base_avg_win_rates),3)\n",
    "cluster_avg_win_rates_mean = round_sig(np.mean(cluster_avg_win_rates),3)\n",
    "cluster_2_avg_win_rates_mean = round_sig(np.mean(cluster_2_avg_win_rates),3)\n",
    "greedy_search_avg_win_rates_mean = round_sig(np.mean(greedy_search_avg_win_rates),3)\n",
    "greedy_search_2_avg_win_rates_mean = round_sig(np.mean(greedy_search_2_avg_win_rates),3)\n",
    "\n",
    "base_avg_win_rates_variance = round_sig(np.var(base_avg_win_rates),3)\n",
    "cluster_avg_win_rates_variance = round_sig(np.var(cluster_avg_win_rates),3)\n",
    "cluster_2_avg_win_rates_variance = round_sig(np.var(cluster_2_avg_win_rates),3)\n",
    "greedy_search_avg_win_rates_variance = round_sig(np.var(greedy_search_avg_win_rates),3)\n",
    "greedy_search_2_avg_win_rates_variance = round_sig(np.var(greedy_search_2_avg_win_rates),3)\n",
    "\n",
    "base_avg_win_rates_CI = calculate_t_test_CI(base_avg_win_rates, 0.95)\n",
    "cluster_avg_win_rates_CI = calculate_t_test_CI(cluster_avg_win_rates, 0.95)\n",
    "cluster_2_avg_win_rates_CI = calculate_t_test_CI(cluster_2_avg_win_rates, 0.95)\n",
    "greedy_search_avg_win_rates_CI = calculate_t_test_CI(greedy_search_avg_win_rates, 0.95)\n",
    "greedy_search_2_avg_win_rates_CI = calculate_t_test_CI(greedy_search_2_avg_win_rates, 0.95)\n",
    "\n",
    "print(f'Steady state average guesses for RL base with 15k words is {base_avg_win_rates_mean}, its variance is {base_avg_win_rates_variance} and 95% confidence interval of the average guess is {base_avg_win_rates_CI}.')\n",
    "print(f'Steady state average guesses for RL with clustering & 15k words is {cluster_avg_win_rates_mean}, its variance is {cluster_avg_win_rates_variance} and 95% confidence interval of the average guess is {cluster_avg_win_rates_CI}.')\n",
    "print(f'Steady state average guesses for RL with clustering & 2k words is {cluster_2_avg_win_rates_mean}, its variance is {cluster_2_avg_win_rates_variance} and 95% confidence interval of the average guess is {cluster_2_avg_win_rates_CI}.')\n",
    "print(f'Steady state average guesses for Greedy search with 15k words is {greedy_search_avg_win_rates_mean}, its variance is {greedy_search_avg_win_rates_variance} and 95% confidence interval of the average guess is {greedy_search_avg_win_rates_CI}.')\n",
    "print(f'Steady state average guesses for Greedy search with 2k words is {greedy_search_2_avg_win_rates_mean}, its variance is {greedy_search_2_avg_win_rates_variance} and 95% confidence interval of the average guess is {greedy_search_2_avg_win_rates_CI}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['time_taken']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_words = []\n",
    "with open('models/goal_words.txt', 'r') as file:\n",
    "    for word in file:\n",
    "        goal_words.append(word.strip('\\n').upper())\n",
    "\n",
    "# Simulate our random selection of goal word for the 10000 runs\n",
    "words = {}\n",
    "for i in range(10000):\n",
    "    word = goal_words[int(np.random.uniform(0,2308))]\n",
    "    if word in words:\n",
    "        words[word] += 1\n",
    "    else:\n",
    "        words[word] = 1\n",
    "\n",
    "# Do a count of the number of times each word was selected and plot\n",
    "df = pd.DataFrame(words.items(), columns=['word', 'count'])\n",
    "df.sort_values(by='count', ascending=False, inplace=True)\n",
    "df = df['count'].value_counts()\n",
    "\n",
    "df.plot(kind='bar')\n",
    "plt.title(\"Distribution of goal words used across 10000 runs\")\n",
    "plt.xlabel(\"Number of times used as goal words\")\n",
    "plt.ylabel(\"Count of goal words\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11c27258e39e959dad06351b7d990b91fa2aa6475255b3c01170298971998ca4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
